# Legalese-Simplifier: Fine-Tuning Llama 3.2 for Legal Accessibility

A professional-grade LLM fine-tuning project that transforms dense, archaic legalese into plain, actionable English using **QLoRA** and **Llama 3.2-3B**.

## ðŸš€ Project Overview
Legal documents are often inaccessible to the general public. This project demonstrates how to specialize a general-purpose Large Language Model for the legal domain. By fine-tuning on a curated dataset of contract clauses, the model learns to maintain legal intent while drastically improving readability.

### Key Technical Features:
- **Base Model:** Llama 3.2-3B-Instruct
- **Fine-tuning Method:** QLoRA (4-bit Quantized Low-Rank Adaptation)
- **Framework:** Unsloth (Optimized for 2x faster training & 70% less VRAM)
- **Dataset:** 400+ high-quality pairs of complex legal clauses vs. simplified equivalents.

---

## ðŸ“‚ Project Structure
```text
legalese-simplifier/
â”œâ”€â”€ .gitignore               # Prevents large model weights from bloating the repo
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ requirements.txt         # Environment dependencies
â”œâ”€â”€ data/
â”‚   â””â”€â”€ legal_train.jsonl    # Specialized legal dataset (JSONL format)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Training_Llama3.ipynb# Colab-ready training pipeline
â”œâ”€â”€ model_cards/
â”‚   â””â”€â”€ adapter_config.json  # PEFT/LoRA configuration details
â””â”€â”€ app/
    â””â”€â”€ main.py              # Gradio-based inference UI